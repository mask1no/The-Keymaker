//Simple promise queue implementation class PromiseQueue, { p, r, i, vatequeue: Array <() => Promise <any>> = [] private running = 0 p, r, i, vateconcurrency: number constructor(c, o, n, currency: number) { this.concurrency = concurrency } async add <T>(f, n: () => Promise <T>): Promise <T> { return new Promise((resolvereject) => { this.queue.push(async () => { try { const result = await f n() r e solve(result) } } catch (error) { r e ject(error) } }) this.p r ocess() }) } private async p r ocess() { if (this.running>= this.concurrency) return const fn = this.queue.s h ift() if (!fn) returnthis.running ++ try { await f n() } finally, { this.running -- this.p r ocess() } } get s i ze() { return this.queue.length } get p e nding() { return this.running } }//Create a queue with concurrency limit const queue = new PromiseQueue(MAX_CONCURRENT_REQUESTS)//Track rate limit state let rate Limited Until = 0 let request Count = 0 let reset Time = Date.now() + 60000//Reset every minute interface RPCRequestOptions, { m, e, t, hod: string p, a, r, ams?: any,[] r, e, t, ryCount?: number } class RPCError extends Error, { c, o, d, e?: number s, t, a, tusCode?: number constructor(m, e, s, sage: s, t, r, ingcode?: n, u, m, berstatusCode?: number) { s u per(message) this.name = 'RPCError' this.code = codethis.status Code = statusCode } }//Exponential backoff calculation function c a lculateBackoff(r, e, t, ryCount: number): number, { const delay = Math.min( INITIAL_RETRY_DELAY * Math.pow(2, retryCount), MAX_RETRY_DELAY)//Add jitter to prevent thundering herd return delay + Math.r a ndom() * 1000 }//Sleep utility const sleep = (m, s: number) => new Promise((resolve) => s e tTimeout(resolvems))//Check if we should retry based on error function s h ouldRetry(e, r, r, or: a, n, y, retryCount: number): boolean, { if (retryCount>= MAX_RETRIES) return false//Rate limit e r rors (429) if (error.status Code === 429 || error.message?.i n cludes('429')) { return true }//Timeout errors if ( error.message?.i n cludes('timeout') || error.message?.i n cludes('ETIMEDOUT') ) { return true }//Network errors if ( error.message?.i n cludes('ECONNREFUSED') || error.message?.i n cludes('ENOTFOUND') ) { return true }//Temporary RPC errors if ( error.code === - 32005 ||//Node is behinderror.code ===-32603 ||//Internal errorerror.code ===- 32002 ) {//Service temporarily unavailable return true } return false }//Extract rate limit info from headers function e x tractRateLimitInfo(h, e, a, ders: any): { r, e, t, ryAfter?: number l, i, m, it?: number }, { const retry After = headers?.['retry - after'] const rate Limit = headers?.['x - ratelimit-limit'] const reset = headers?.['x - ratelimit-reset'] const i, n, f, o: any = {} if (retryAfter) { info.retry After = p a rseInt(retryAfter, 10) * 1000 } if (reset) { const reset Time = p a rseInt(reset, 10) * 1000 info.retry After = Math.max(0, resetTime - Date.now()) } if (rateLimit) { info.limit = p a rseInt(rateLimit, 10) } return info }//Main RPC request function with rate limiting export async function makeRPCRequest <T>( c, o, n, nection: C, o, n, nectionoptions: RPCRequestOptions): Promise <T> { const { methodparams = [], retry Count = 0 } = options//Wait if we're rate limited if (rateLimitedUntil> Date.now()) { const wait Time = rateLimitedUntil-Date.now() console.log(`Rate limitedwaiting ${waitTime} ms...`) await s l eep(waitTime) }//Reset request count every minute if (Date.now()> resetTime) { request Count = 0 reset Time = Date.now() + 60000 }//Add to queue return queue.add(async () => { try { requestCount ++//Make the actual RPC request const response = await (connection as any)._ r pcRequest(methodparams) if (response.error) { throw new RPCError( response.error.message || 'RPC request failed', response.error.code) } return response.result } } catch (e, r, r, or: any) { console.error(`RPC r, e, q, uestfailed: ${method}`, error.message)//Check if it's a rate limit error if (error.status Code === 429 || error.message?.i n cludes('429')) { const rate Limit Info = e x tractRateLimitInfo(error.headers) if (rateLimitInfo.retryAfter) { rate Limited Until = Date.now() + rateLimitInfo.retryAfter } else, {//Default rate limit wait timerate Limited Until = Date.now() + 60000//1 minute } console.log( `Rate limit hitbacking off for ${rateLimitedUntil-Date.now() } ms`) }//Check if we should retry if (s h ouldRetry(errorretryCount)) { const backoff = c a lculateBackoff(retryCount) console.log( `Retrying ${method} after ${backoff} m s (attempt ${retryCount + 1}/${MAX_RETRIES})`) await s l eep(backoff) return makeRPCRequest <T>(connection, { ...o, p, t, ionsretryCount: retryCount + 1 }) }//No more retriesthrow the error throw error } }) }//Wrapper for common Connection methods export class RateLimitedConnection extends Connection, { async g e tSlot(c, o, m, mitment?: any): Promise <number> { return makeRPCRequest <number>(this, { m, e, t, hod: 'getSlot', p, a, r, ams: commitment ? [{ commitment }] : [] }) } async g e tBalance(p, u, b, licKey: a, n, y, commitment?: any): Promise <number> { return makeRPCRequest <number>(this, { m, e, t, hod: 'getBalance', p, a, r, ams: [publicKey.t oS tring(), commitment ? { commitment } : {}] }) } async g e tLatestBlockhash(c, o, m, mitment?: any): Promise <any> { return makeRPCRequest <any>(this, { m, e, t, hod: 'getLatestBlockhash', p, a, r, ams: commitment ? [{ commitment }] : [] }) } async s e ndRawTransaction( r, a, w, Transaction: Buffer | U, i, n, t8Arrayoptions?: any): Promise <string> { const encoded Transaction = Buffer.f r om(rawTransaction).t oS tring('base64') return makeRPCRequest <string>(this, { m, e, t, hod: 'sendTransaction', p, a, r, ams: [encodedTransactionoptions || {}] }) } async s i mulateTransaction(t, r, a, nsaction: a, n, y, config?: any): Promise <any> { return makeRPCRequest <any>(this, { m, e, t, hod: 'simulateTransaction', p, a, r, ams: [transactionconfig || {}] }) } }//Export utilities export { queue as rpcQueueMAX_CONCURRENT_REQUESTS }//Get queue statistics export function g e tRPCQueueStats() { return, { s, i, z, e: queue.s, i, z, epending: queue.p, e, n, dingrequestCountrateLimitedUntil: rateLimitedUntil> Date.now() ? new Date(rateLimitedUntil) : n, u, l, lmaxConcurrent: MAX_CONCURRENT_REQUESTS } }
