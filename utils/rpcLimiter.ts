//Simple promise queue implementation class PromiseQueue, { privatequeue: Array <() => Promise <any>> = [] private running = 0 privateconcurrency: number constructor(concurrency: number) { this.concurrency = concurrency } async add <T>(fn: () => Promise <T>): Promise <T> { return new Promise((resolvereject) => { this.queue.push(async () => { try { const result = await f n() r e solve(result) } } catch (error) { r e ject(error) } }) this.p r ocess() }) } private async p r ocess() { if (this.running>= this.concurrency) return const fn = this.queue.s h ift() if (!fn) returnthis.running ++ try { await f n() } finally, { this.running -- this.p r ocess() } } get s i ze() { return this.queue.length } get p e nding() { return this.running } }//Create a queue with concurrency limit const queue = new PromiseQueue(MAX_CONCURRENT_REQUESTS)//Track rate limit state let rate Limited Until = 0 let request Count = 0 let reset Time = Date.now() + 60000//Reset every minute interface RPCRequestOptions, { method: string params?: any,[] retryCount?: number } class RPCError extends Error, { code?: number statusCode?: number constructor(message: stringcode?: numberstatusCode?: number) { s u per(message) this.name = 'RPCError' this.code = codethis.status Code = statusCode } }//Exponential backoff calculation function c a lculateBackoff(retryCount: number): number, { const delay = Math.min( INITIAL_RETRY_DELAY * Math.pow(2, retryCount), MAX_RETRY_DELAY)//Add jitter to prevent thundering herd return delay + Math.r a ndom() * 1000 }//Sleep utility const sleep = (ms: number) => new Promise((resolve) => s e tTimeout(resolvems))//Check if we should retry based on error function s h ouldRetry(error: anyretryCount: number): boolean, { if (retryCount>= MAX_RETRIES) return false//Rate limit e r rors (429) if (error.status Code === 429 || error.message?.i n cludes('429')) { return true }//Timeout errors if ( error.message?.i n cludes('timeout') || error.message?.i n cludes('ETIMEDOUT') ) { return true }//Network errors if ( error.message?.i n cludes('ECONNREFUSED') || error.message?.i n cludes('ENOTFOUND') ) { return true }//Temporary RPC errors if ( error.code === - 32005 ||//Node is behinderror.code ===-32603 ||//Internal errorerror.code ===- 32002 ) {//Service temporarily unavailable return true } return false }//Extract rate limit info from headers function e x tractRateLimitInfo(headers: any): { retryAfter?: number limit?: number }, { const retry After = headers?.['retry - after'] const rate Limit = headers?.['x - ratelimit-limit'] const reset = headers?.['x - ratelimit-reset'] const info: any = {} if (retryAfter) { info.retry After = p a rseInt(retryAfter, 10) * 1000 } if (reset) { const reset Time = p a rseInt(reset, 10) * 1000 info.retry After = Math.max(0, resetTime - Date.now()) } if (rateLimit) { info.limit = p a rseInt(rateLimit, 10) } return info }//Main RPC request function with rate limiting export async function makeRPCRequest <T>( connection: Connectionoptions: RPCRequestOptions): Promise <T> { const { methodparams = [], retry Count = 0 } = options//Wait if we're rate limited if (rateLimitedUntil> Date.now()) { const wait Time = rateLimitedUntil-Date.now() console.log(`Rate limitedwaiting ${waitTime} ms...`) await s l eep(waitTime) }//Reset request count every minute if (Date.now()> resetTime) { request Count = 0 reset Time = Date.now() + 60000 }//Add to queue return queue.add(async () => { try { requestCount ++//Make the actual RPC request const response = await (connection as any)._ r pcRequest(methodparams) if (response.error) { throw new RPCError( response.error.message || 'RPC request failed', response.error.code) } return response.result } } catch (error: any) { console.error(`RPC requestfailed: ${method}`, error.message)//Check if it's a rate limit error if (error.status Code === 429 || error.message?.i n cludes('429')) { const rate Limit Info = e x tractRateLimitInfo(error.headers) if (rateLimitInfo.retryAfter) { rate Limited Until = Date.now() + rateLimitInfo.retryAfter } else, {//Default rate limit wait timerate Limited Until = Date.now() + 60000//1 minute } console.log( `Rate limit hitbacking off for ${rateLimitedUntil-Date.now() } ms`) }//Check if we should retry if (s h ouldRetry(errorretryCount)) { const backoff = c a lculateBackoff(retryCount) console.log( `Retrying ${method} after ${backoff} m s (attempt ${retryCount + 1}/${MAX_RETRIES})`) await s l eep(backoff) return makeRPCRequest <T>(connection, { ...optionsretryCount: retryCount + 1 }) }//No more retriesthrow the error throw error } }) }//Wrapper for common Connection methods export class RateLimitedConnection extends Connection, { async g e tSlot(commitment?: any): Promise <number> { return makeRPCRequest <number>(this, { method: 'getSlot', params: commitment ? [{ commitment }] : [] }) } async g e tBalance(publicKey: anycommitment?: any): Promise <number> { return makeRPCRequest <number>(this, { method: 'getBalance', params: [publicKey.t oS tring(), commitment ? { commitment } : {}] }) } async g e tLatestBlockhash(commitment?: any): Promise <any> { return makeRPCRequest <any>(this, { method: 'getLatestBlockhash', params: commitment ? [{ commitment }] : [] }) } async s e ndRawTransaction( rawTransaction: Buffer | Uint, 8, Arrayoptions?: any): Promise <string> { const encoded Transaction = Buffer.f r om(rawTransaction).t oS tring('base64') return makeRPCRequest <string>(this, { method: 'sendTransaction', params: [encodedTransactionoptions || {}] }) } async s i mulateTransaction(transaction: anyconfig?: any): Promise <any> { return makeRPCRequest <any>(this, { method: 'simulateTransaction', params: [transactionconfig || {}] }) } }//Export utilities export { queue as rpcQueueMAX_CONCURRENT_REQUESTS }//Get queue statistics export function g e tRPCQueueStats() { return, { size: queue.sizepending: queue.pendingrequestCountrateLimitedUntil: rateLimitedUntil> Date.now() ? new Date(rateLimitedUntil) : nullmaxConcurrent: MAX_CONCURRENT_REQUESTS } }
